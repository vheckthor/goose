"use strict";(self.webpackChunkgoose=self.webpackChunkgoose||[]).push([[1169],{37834:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>g,frontMatter:()=>a,metadata:()=>n,toc:()=>l});var n=s(3744),o=s(74848),i=s(28453);const a={title:"3 Prompts to Test for Agent Readiness",description:"A series of prompts to test an LLM's capabilities to be used with AI agents",authors:["angie"]},r=void 0,c={authorsImageUrls:[void 0]},l=[];function p(e){const t={a:"a",em:"em",img:"img",p:"p",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"blog cover",src:s(71078).A+"",width:"1200",height:"630"})}),"\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.a,{href:"/",children:"Goose"})," is LLM-agnostic, meaning you can plug in the model of your choice. However, not every LLM is suitable to work with agents. Some may be great at ",(0,o.jsx)(t.em,{children:"answering"})," things, but not actually ",(0,o.jsx)(t.em,{children:"doing"})," things. If you're considering which model to use with an agent, these 3 prompts can quickly give you a sense of the model's capabilities."]})]})}function g(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},71078:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/llm-agent-test-86ce2379ce4dde48ae1448f0f9d75c1f.png"},28453:(e,t,s)=>{s.d(t,{R:()=>a,x:()=>r});var n=s(96540);const o={},i=n.createContext(o);function a(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),n.createElement(i.Provider,{value:t},e.children)}},3744:e=>{e.exports=JSON.parse('{"permalink":"/goose/pr-preview/pr-2742/blog/2025/05/22/llm-agent-readiness","source":"@site/blog/2025-05-22-llm-agent-readiness/index.md","title":"3 Prompts to Test for Agent Readiness","description":"A series of prompts to test an LLM\'s capabilities to be used with AI agents","date":"2025-05-22T00:00:00.000Z","tags":[],"readingTime":2.325,"hasTruncateMarker":true,"authors":[{"name":"Angie Jones","title":"Head of Developer Relations","url":"https://angiejones.tech","page":{"permalink":"/goose/pr-preview/pr-2742/blog/authors/angie"},"socials":{"linkedin":"https://www.linkedin.com/in/angiejones/","github":"https://github.com/angiejones","x":"https://x.com/techgirl1908","bluesky":"https://bsky.app/profile/angiejones.tech"},"imageURL":"https://avatars.githubusercontent.com/u/15972783?v=4","key":"angie"}],"frontMatter":{"title":"3 Prompts to Test for Agent Readiness","description":"A series of prompts to test an LLM\'s capabilities to be used with AI agents","authors":["angie"]},"unlisted":false,"nextItem":{"title":"How I Manage Localhost Port Conflicts With an AI Agent","permalink":"/goose/pr-preview/pr-2742/blog/2025/05/22/manage-local-host-conflicts-with-goose"}}')}}]);